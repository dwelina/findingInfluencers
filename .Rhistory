42.17+4.2+8.40
(86.4*30)/1000
(86.4*21)/1000
(86.4*25)/1000
(86.4*22)/1000
(2*(10^9))/500
((2*(10^9))/500)/(24*60*60*2)
23/7
(50*3,05)+85
(50*3.05)+85
((50*3.05)+85)/50
(((50*3.05)+85)*1.24)/50
(((50*3.05)+85)*1.24)
library(lubridate)
quarter(Sys.Date())
time <- "April"
sheetname <- paste("2017:Q1_2", time)
sheetname
?paste
paste("2017:Q1_2", time, sep="")
paste("2017:", time, sep="")
sheetname <- time
sheetname
library(googlesheets)
?gs_copy
?gs_auth
gs_auth(new_user = TRUE)
gs_ls()
sheetit <- gs_ls()
View(sheetit)
gs_copy(from = "Tulospöytäkirja", to = "TulospöytäkirjaCopy")
tt <- gs_title("Tulospöytäkirja")
tt <- sheetit[1,1]
tt
as.character(tt)
GStt <- gs_title(as.character(tt))
gs_copy(from = GStt, to = "TulospöytäkirjaCopy")
Sys.setlocale("LC_ALL", "pt_PT.ISO8859-1")
sheetit <- gs_ls()
View(sheetit)
sheetit <- gs_ls()
gs_read(sheetit[1,1])
gs_read(as.character(sheetit[1,1]))
sheetit[1,1]
gs_read(ws = as.character(sheetit[1,1]))
?gs_read
gs_read(ss = as.character(sheetit[1,1]))
gs_read(ss = as.character(sheetit[1,1]), ws=1)
sheetit[1,1]
as.character(sheetit[1,1])
title <- gs_title(as.character(sheetit[1,1]))
gs_read(ss = title, ws=1)
osakilpailut <- gs_read(ss = title, ws=1)
View(osakilpailut)
gs_read(GStt)
lue_testi <- gs_read(GStt)
View(lue_testi)
GStt
View(sheetit)
tulosCopy <- gs_title(as.character(sheetit[2,1]))
kissa <- gs_copy(from = tulosCopy)
gs_ws_new(ss = tulosCopy, ws_title = "tulosCopy", input = gs_copy(from = tulosCopy))
title2 <- gs_title(as.character(sheetit[3,1]))
View(osakilpailut)
#Tallenetaan salit
salit <- unique(osakilpailut$Sali)
no_of_salit <- length(salit)
#Loopataan kaikki salit läpi
foreach(j=1:no_of_salit) %do% {
filename <- as.character(salit[j])
osakilpailut_ss <- subset(osakilpailut, Sali == salit[j])
#Kopioidaan pohjasta ja nimetään uusiksi
gs_copy(from = title2, to = filename)
gs_ws_rename(to = as.character(osakilpailut_ss$Päivämäärä[1]))
}
library(foreach)
#Loopataan kaikki salit läpi
foreach(j=1:no_of_salit) %do% {
filename <- as.character(salit[j])
osakilpailut_ss <- subset(osakilpailut, Sali == salit[j])
#Kopioidaan pohjasta ja nimetään uusiksi
gs_copy(from = title2, to = filename)
gs_ws_rename(to = as.character(osakilpailut_ss$Päivämäärä[1]))
}
#Loopataan kaikki salit läpi
foreach(j=1:no_of_salit) %do% {
filename <- as.character(salit[j])
osakilpailut_ss <- subset(osakilpailut, Sali == salit[j])
#Kopioidaan pohjasta ja nimetään uusiksi
gs_copy(from = title2, to = filename)
gs_ws_rename(to = as.character(osakilpailut_ss$Päivämäärä[1])
}
colnames(osakilpailut)
colnames(osakilpailut) <- c("Sali", "Pvm", "Aika")
#Loopataan kaikki salit läpi
foreach(j=1:no_of_salit) %do% {
filename <- as.character(salit[j])
osakilpailut_ss <- subset(osakilpailut, Sali == salit[j])
#Kopioidaan pohjasta ja nimetään uusiksi
gs_copy(from = title2, to = filename)
gs_ws_rename(to = as.character(osakilpailut_ss$Pvm[1])
}
#Loopataan kaikki salit läpi
foreach(j=1:no_of_salit) %do% {
filename <- as.character(salit[j])
osakilpailut_ss <- subset(osakilpailut, Sali == salit[j])
#Kopioidaan pohjasta ja nimetään uusiksi
gs_copy(from = title2, to = filename)
gs_ws_rename(to = as.character(osakilpailut_ss$Pvm[1]))
}
View(osakilpailut_ss)
osakilpailut_ss$Pvm[1])
osakilpailut_ss$Pvm[1]
?gs_ws_rename
gs_ws_rename(filename, to = as.character(osakilpailut_ss$Pvm[1]))
gs_ws_rename(gs_title(filename), to = as.character(osakilpailut_ss$Pvm[1]))
#Tallenetaan salit
salit <- unique(osakilpailut$Sali)
no_of_salit <- length(salit)
#Loopataan kaikki salit läpi
foreach(j=1:no_of_salit) %do% {
filename <- paste("Ghetto Snatch osakilpailut 2017:", as.character(salit[j]))
osakilpailut_ss <- subset(osakilpailut, Sali == salit[j])
#Kopioidaan pohjasta ja nimetään uusiksi
gs_copy(from = title2, to = filename)
valilehti <- paste("Osakilpailu", as.character(osakilpailut_ss$Pvm[1]))
gs_ws_rename(gs_title(filename), to = valilehti)
}
Sys.setlocale("LC_ALL", "pt_PT.ISO8859-1")
#Kirjastot
library(googlesheets)
library(foreach)
#Valitse pcraisan
gs_auth(new_user = TRUE)
#Haetaan kaikki sheetit
sheetit <- gs_ls()
View(sheetit)
title1 <- gs_title(as.character(sheetit[9,1]))
osakilpailut <- gs_read(ss = title1, ws=1)
View(osakilpailut)
title2 <- gs_title(as.character(sheetit[2,1]))
#Tallenetaan salit
salit <- unique(osakilpailut$Sali)
no_of_salit <- length(salit)
#Loopataan kaikki salit lÃ¤pi
foreach(j=1:no_of_salit) %do% {
filename <- paste("Ghetto Snatch osakilpailut 2017:", as.character(salit[j]))
osakilpailut_ss <- subset(osakilpailut, Sali == salit[j])
#Kopioidaan pohjasta ja nimetÃ¤Ã¤n uusiksi
gs_copy(from = title2, to = filename)
valilehti <- paste("Osakilpailu", as.character(osakilpailut_ss$Pvm[1]))
gs_ws_rename(gs_title(filename), to = valilehti)
}
?gs_ws_rename
sheetit <- gs_ls()
View(sheetit)
title2 <- gs_title(as.character(sheetit[1,1]))
#--------automaattista tÃ¤stÃ¤ eteenpÃ¤in-----------
#Tallenetaan salit
salit <- unique(osakilpailut$Sali)
no_of_salit <- length(salit)
#Loopataan kaikki salit lÃ¤pi
foreach(j=1:no_of_salit) %do% {
filename <- paste("Ghetto Snatch osakilpailut 2017:", as.character(salit[j]))
osakilpailut_ss <- subset(osakilpailut, Sali == salit[j])
#Kopioidaan pohjasta ja nimetÃ¤Ã¤n uusiksi
gs_copy(from = title2, to = filename)
valilehti1 <- paste("Tulokset", as.character(osakilpailut_ss$Pvm[1]))
gs_ws_rename(gs_title(filename),from = 1, to = valilehti1)
valilehti2 <- paste("Punnitus", as.character(osakilpailut_ss$Pvm[1]))
gs_ws_rename(gs_title(filename),from = 2, to = valilehti2)
}
View(osakilpailut)
#Connecting to dashDB on Bluemix
library(RJDBC)
#Connect to dashDB database
message("Creating database connection")
driverName <- "com.ibm.db2.jcc.DB2Driver"
pkgPath <- "/Applications/dsdriver/java/db2jcc4.jar"
connectionString <- "jdbc:db2://dashdb-entry-yp-dal09-10.services.dal.bluemix.net:50000/BLUDB"
userName <- "dash7660"
password <- "n6acnLUDykoO"
drv <- JDBC(driverName, pkgPath, identifier.quote="'")
conn <- dbConnect(drv, connectionString, userName, password)
#get sensor data
dataSensor <- dbGetQuery(conn, "SELECT * FROM SENSOR_DATA")
dataParveke <- dbGetQuery(conn, "SELECT * FROM PARVEKE")
View(dataParveke)
install.packages("RPostgreSQL")
l
library("RPostgreSQL")
drv = dbDriver("PostgreSQL")
# Local Postgres.app database; no password by default
# Of course, you fill in your own database information here.
con = dbConnect(drv, user="ninazumel", password="",
host="localhost", port=5432, dbname="pcraisan")
con = dbConnect(drv, user="pcraisan", password="",
host="localhost", port=5432, dbname="pcraisan")
?dbListTables()
?dbListTables
dbListTables(con)
data <- dbGetQuery(con, "select * from youtube_video_example")
View(data)
unique(data$channel_id)
nrow(unique(data$channel_id))
count(unique(data$channel_id))
length(unique(data$channel_id))
data$id[1]
data$id[5]
install.packages("slam")
install.packages("topicmodels")
library(“tm”)
library(“wordcloud”)
library(“slam”)
library(“topicmodels”)
library("tm")
library("wordcloud")
library("slam")
library("topicmodels")
tweets <- data$description
#Clean Text
tweets = gsub("(RT|via)((?:\b\W*@\w+)+)","",tweets)
tweets = gsub("http[^[:blank:]]+", "", tweets)
tweets = gsub("@\w+", "", tweets)
tweets = gsub("[ t]{2,}", "", tweets)
tweets = gsub("^\s+|\s+$", "", tweets)
tweets <- gsub(‘\d+’, ", tweets)
tweets = gsub("[[:punct:]]", " ", tweets)
corpus = Corpus(VectorSource(tweets))
d <- structure(list(ID = c("KP1009", "GP3040", "KP1757", "GP2243",
"KP682", "KP1789", "KP1933", "KP1662", "KP1718", "GP3339", "GP4007",
"GP3398", "GP6720", "KP808", "KP1154", "KP748", "GP4263", "GP1132",
"GP5881", "GP6291", "KP1004", "KP1998", "GP4123", "GP5930", "KP1070",
"KP905", "KP579", "KP1100", "KP587", "GP913", "GP4864", "KP1513",
"GP5979", "KP730", "KP1412", "KP615", "KP1315", "KP993", "GP1521",
"KP1034", "KP651", "GP2876", "GP4715", "GP5056", "GP555", "GP408",
"GP4217", "GP641"),
Type = c("B", "A", "B", "A", "B", "B", "B",
"B", "B", "A", "A", "A", "A", "B", "B", "B", "A", "A", "A", "A",
"B", "B", "A", "A", "B", "B", "B", "B", "B", "A", "A", "B", "A",
"B", "B", "B", "B", "B", "A", "B", "B", "A", "A", "A", "A", "A",
"A", "A"),
Set = c(15L, 1L, 10L, 21L, 5L, 9L, 12L, 15L, 16L,
19L, 22L, 3L, 12L, 22L, 15L, 25L, 10L, 25L, 12L, 3L, 10L, 8L,
8L, 20L, 20L, 19L, 25L, 15L, 6L, 21L, 9L, 5L, 24L, 9L, 20L, 5L,
2L, 2L, 11L, 9L, 16L, 10L, 21L, 4L, 1L, 8L, 5L, 11L), Loc = c(3L,
2L, 3L, 1L, 3L, 3L, 3L, 1L, 2L, 1L, 3L, 1L, 1L, 2L, 2L, 1L, 3L,
2L, 2L, 2L, 3L, 2L, 3L, 2L, 1L, 3L, 3L, 3L, 2L, 3L, 1L, 3L, 3L,
1L, 3L, 2L, 3L, 1L, 1L, 1L, 2L, 3L, 3L, 3L, 2L, 2L, 3L, 3L)),
.Names = c("ID", "Type", "Set", "Loc"), class = "data.frame",
row.names = c(NA, -48L))
View(d)
library(igraph)
head(nodes2)
?rep
channel <- c("ch1", "ch1", "ch2", "ch3", rep("ch4", 3))
channel
channel <- c("ch1", "ch1", "ch2", "ch3", rep("ch4", 3), rep("ch5", 3), "ch6")
topic <- c("t1", "t2", "t1", "t1", "t3", "t1", "t2", "t3", "t1", "t2", "t3", "t2")
channel <- c("ch1", "ch1", "ch2", rep("ch3", 2), rep("ch4", 3), rep("ch5", 3), "ch6")
data <- cbind(channel, topic)
View(data)
library(reshape2)
table(melt(data, id.var="channel")[-1])
library(reshape2)
table(melt(data, id.var="channel")[-2])
table(data)
View(data)
data <- as.data.frame(data)
table(data)
links2 <- table(data)
net2 <- graph_from_incidence_matrix(links2)
net2.bp <- bipartite.projection(net2)
channel <- unique(data$channel)
id <- unique(data$channel)
media.type <- c(1,1,1,2,1,2)
nodes2 <- cbind(id, media.type)
View(nodes2)
id
id <- as.data.frame(id)
id
nodes2 <- cbind(id, media.type)
plot(net2.bp$proj1, vertex.label.color="black", vertex.label.dist=1,
vertex.size=7, vertex.label=nodes2$media[!is.na(nodes2$media.type)])
###################################################
#Finding Influencers in YouTube data
#Code by Paula Räisänen
###################################################
#Uncomment to install required packages
# install.packages("RPostgreSQL")
#Import packages
library(RPostgreSQL)
library(tuber)
library(qdab)
library(readr)
#Connect to PostgreSQL
drv <- dbDriver("PostgreSQL")
conn <- dbConnect(drv, user="pcraisan", password="", host="localhost", port=5432, dbname="pcraisan")
#Querying YouTube data
data <- dbGetQuery(conn, "select * from youtube_video_example")
#No need to hold the connection open so closing it as this point
dbDisconnect(conn)
#Get a subset of the data
data_ss <- head(data, 1000)
#Hoping to do some text mining on the data so will need to clean unwanted things
data_ss$description <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", data_ss$description)
#data_ss$description <- gsub("[[:punct:]]", "", data_ss$description)
#data_ss$description  <- gsub("@\\w+ *", "", data_ss$description)
data_ss$description <- Trim(clean(data_ss$description))
library(qdab)
library(qdap)
data_ss$description <- Trim(clean(data_ss$description))
questions <- cbind(data_ss$id, data_ss$description)
colnames(questions) <- c("Id", "Body")
library(dplyr)
question_counts <- questions %>%
mutate(Body = str_replace_all(Body, "<[^>]*>", "")) %>% ## remove HTML tags
unnest_tokens(Word, Body) %>%
filter(str_detect(Word, "^[a-z]")) %>%
anti_join(my_stop_words,
by = c("Word" = "word")) %>%
count(Id, Word, sort = TRUE)
questions <- as.data.frame(questions)
question_counts <- questions %>%
mutate(Body = str_replace_all(Body, "<[^>]*>", "")) %>% ## remove HTML tags
unnest_tokens(Word, Body) %>%
filter(str_detect(Word, "^[a-z]")) %>%
anti_join(my_stop_words,
by = c("Word" = "word")) %>%
count(Id, Word, sort = TRUE)
question_counts <- questions %>%
mutate(Body = str_replace_all(Body, "<[^>]*>", "")) %>% ## remove HTML tags
unnest_tokens(Word, Body) %>%
filter(str_detect(Word, "^[a-z]")) %>%
anti_join(my_stop_words,
by = c("Word" = "word")) %>%
count(Id, Word, sort = TRUE)
library(stringr)
question_counts <- questions %>%
mutate(Body = str_replace_all(Body, "<[^>]*>", "")) %>% ## remove HTML tags
unnest_tokens(Word, Body) %>%
filter(str_detect(Word, "^[a-z]")) %>%
anti_join(my_stop_words,
by = c("Word" = "word")) %>%
count(Id, Word, sort = TRUE)
library(tidytext)
question_counts <- questions %>%
mutate(Body = str_replace_all(Body, "<[^>]*>", "")) %>% ## remove HTML tags
unnest_tokens(Word, Body) %>%
filter(str_detect(Word, "^[a-z]")) %>%
anti_join(my_stop_words,
by = c("Word" = "word")) %>%
count(Id, Word, sort = TRUE)
my_stop_words <- bind_rows(stop_words %>%
filter(lexicon == "snowball"),
data_frame(word = c("gt", "lt"),
lexicon = rep("custom", 2)))
question_counts <- questions %>%
mutate(Body = str_replace_all(Body, "<[^>]*>", "")) %>% ## remove HTML tags
unnest_tokens(Word, Body) %>%
filter(str_detect(Word, "^[a-z]")) %>%
anti_join(my_stop_words,
by = c("Word" = "word")) %>%
count(Id, Word, sort = TRUE)
library(topicmodels)
question_lda <- LDA(question_dtm, k = 6, control = list(seed = 1234))
question_dtm <- question_counts %>%
cast_dtm(Id, Word, n
)
question_lda <- LDA(question_dtm, k = 6, control = list(seed = 1234))
View(question_counts)
question_dtm <- question_counts %>%
cast_dtm(Id, Word, n)
question_lda <- LDA(question_dtm, k = 6, control = list(seed = 1234))
question_lda <- LDA(question_dtm, k = 6, control = list(seed = 123))
question_lda <- LDA(question_dtm, k = 12, control = list(seed = 123))
View(question_counts)
question_dtm
View(questions)
questions <- questions[complete.cases(questions), ]
View(questions)
questions <- na.omit(questions)
questions$Body[1]
questions <- cbind(data_ss$id, data_ss$description)
questions <- na.omit(questions)
questions <- cbind(data_ss$id, data_ss$description)
questions <- na.omit(questions)
questions <- as.data.frame(questions)
questions <- na.omit(questions)
questions <- subset(questions, Body != "")
colnames(questions) <- c("Id", "Body")
questions <- subset(questions, Body != "")
my_stop_words <- bind_rows(stop_words %>%
filter(lexicon == "snowball"),
data_frame(word = c("gt", "lt"),
lexicon = rep("custom", 2)))
question_counts <- questions %>%
mutate(Body = str_replace_all(Body, "<[^>]*>", "")) %>% ## remove HTML tags
unnest_tokens(Word, Body) %>%
filter(str_detect(Word, "^[a-z]")) %>%
anti_join(my_stop_words,
by = c("Word" = "word")) %>%
count(Id, Word, sort = TRUE)
question_dtm <- question_counts %>%
cast_dtm(Id, Word, n)
question_lda <- LDA(question_dtm, k = 6, control = list(seed = 1234))
rowTotals <- apply(question_dtm , 1, sum) #Find the sum of words in each Document
question_dtm.new   <- question_dtm[rowTotals> 0, ]
question_lda <- LDA(question_dtm.new, k = 6, control = list(seed = 1234))
library(ggplot2)
tidy_lda <- tidy(question_lda)
top_terms <- tidy_lda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(topic = factor(topic, labels = str_c("topic ", 1:6)),
term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x),
expand = c(0,0)) +
scale_y_continuous(expand = c(0,0)) +
labs(title = "Top terms in each LDA topic",
x = NULL, y = expression(beta)) +
facet_wrap(~ topic, ncol = 4, scales = "free")
View(questions)
View(top_terms)
View(tidy_lda)
library(dplyr)
library(tidytext)
library(readr)
library(widyr)
library(ggraph)
library(igraph)
install.packages("widyr")
install.packages("ggraph")
library(dplyr)
library(tidytext)
library(readr)
library(widyr)
library(ggraph)
library(igraph)
data <- read_csv("/Users/pcraisan/Desktop/rwds/Shiny/findingInfluencers/youtube_dataEN.csv")
View(data)
data <- read_csv("/Users/pcraisan/Desktop/rwds/Shiny/findingInfluencers/youtube_dataEN.csv")
View(data)
# Get channel name and tags
channel_tokens <- data[, c(13,10)]
channel_tokens$tags <- gsub("\"", " ", channel_tokens$tags)
channel_tokens$tags <- gsub(",", " ", channel_tokens$tags)
channel_tokens <- channel_tokens %>%
unnest_tokens(tag, tags)
tokens_by_channel <- channel_tokens %>%
count(channel_title, tag, sort = TRUE) %>%
ungroup()
channel_cors <- tokens_by_channel %>%
pairwise_cor(channel_title, tag, n, sort = TRUE)
head(channel_cors)
set.seed(200)
channel_cors %>%
filter(correlation > .5) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(alpha = correlation, width = correlation)) +
geom_node_point(size = 6, color = "lightblue") +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
getwd()
setwd("./Desktop/P")
setwd("./Desktop/P/match")
setwd("./Desktop/P/matchmade")
setwd("./matchmade/findingInfluencers/")
ls
library(shiny)
rumApp("influencer_app/")
runApp("influencer_app/")
runApp("influencer_app/")
View(channel_cors)
View(tokens_by_channel)
View(data)
View(tidy_lda)
runApp("influencer_app/")
View(data)
?functions
?function
s
?functions
runApp("influencer_app/")
runApp("influencer_app/")
runApp("influencer_app/")
runApp("influencer_app/")
?subset
View(token_counts)
View(data)
runApp("influencer_app/")
runApp("influencer_app/")
runApp("influencer_app/")
runApp("influencer_app/")
runApp("influencer_app/")
runApp("influencer_app/")
runApp("influencer_app/")
colnames(data)
runApp("influencer_app/")
runApp("influencer_app/")
